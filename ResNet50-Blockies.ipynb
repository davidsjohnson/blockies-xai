{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yi0pAex6RNDU"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import tarfile\n",
        "import random\n",
        "\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxTk1ctGpvnW"
      },
      "source": [
        "# Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SLpJb-H9pxs_"
      },
      "outputs": [],
      "source": [
        "CLASSES = ['Healthy', 'OC Degeneration']\n",
        "\n",
        "load_checkpoints = True\n",
        "\n",
        "modeltype = 'mobilenet'\n",
        "\n",
        "ds = 'sick_ones_bendbias_v3_2class_normal'\n",
        "eval_ds = 'sick_ones_bendbias_v3_2class_variation'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua-n5ADsde6b"
      },
      "source": [
        "# Setup and Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSBMiJ-ARVcd",
        "outputId": "01763765-0e0e-4c50-f430-7c4507cc7359"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('two4two_sickones_models_pytorch')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relative_model_path = \"two4two_sickones_models_pytorch\"\n",
        "base_path = Path('./') / relative_model_path\n",
        "base_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPi0wvbicrct"
      },
      "outputs": [],
      "source": [
        "# data downloading and dataset utilities\n",
        "\n",
        "def download_file(url, file_name, cache_dir=\"data\", extract=True, force_download=False, archive_folder=None):\n",
        "    # Ensure the cache directory exists\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    file_path = os.path.join(cache_dir, file_name)\n",
        "\n",
        "    # Download the file\n",
        "    if not os.path.exists(file_path) or force_download:\n",
        "      torch.hub.download_url_to_file(url, file_path)\n",
        "      print(f\"File downloaded to: {file_path}\")\n",
        "    else:\n",
        "      print(f\"File already exists at: {file_path}\")\n",
        "\n",
        "    if extract:\n",
        "      with tarfile.open(file_path, \"r:gz\") as tar:\n",
        "          tar.extractall(path=cache_dir)\n",
        "      print(f\"File extracted to: {cache_dir}\")\n",
        "      return Path(cache_dir) / archive_folder if archive_folder is not None else Path(cache_dir)\n",
        "    elif archive_folder is not None and (Path(cache_dir) / archive_folder).exsists:\n",
        "      return Path(cache_dir) / archive_folder\n",
        "    else:\n",
        "      return Path(cache_dir)\n",
        "\n",
        "    return Path(file_path)\n",
        "\n",
        "def load_dataframe(data_dir, dataset):\n",
        "  data_dir = data_dir / dataset\n",
        "  df = pd.read_json(data_dir / 'parameters.jsonl', lines=True)\n",
        "  df['filename'] = df['id'] + '.png'\n",
        "  df['ill'] = df['ill'].astype(int).astype(str)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, data_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(str(self.data_dir), str(self.df.iloc[idx]['filename'])) ## Added str \n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = int(self.df.iloc[idx]['ill'])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReNR_Y0QProQ"
      },
      "source": [
        "## Load Dataset and Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a1B47gWYgVG",
        "outputId": "a085949b-646a-403b-db41-3a9b12cdfd77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File already exists at: data/two4two_datasets.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_666025/257083907.py:17: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=cache_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File extracted to: data\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PosixPath('data/two4two_datasets')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download datafrom sciebo\n",
        "\n",
        "data_dir = download_file(\"https://uni-bielefeld.sciebo.de/s/2BgY19ixIaEUOmS/download\",\n",
        "                         \"two4two_datasets.tar.gz\",\n",
        "                         cache_dir='data',\n",
        "                         extract=True,\n",
        "                         force_download=False,\n",
        "                         archive_folder='two4two_datasets')\n",
        "data_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn3D8znPQN-F",
        "outputId": "837838bd-ecf5-4b49-f6ac-94a4e5d5ad5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('data/two4two_datasets/sick_ones_bendbias_v3_2class_normal'),\n",
              " PosixPath('data/two4two_datasets/sick_ones_bendbias_v3_2class_variation'))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_dir = data_dir / ds\n",
        "eval_ds_dir = data_dir / eval_ds\n",
        "ds_dir, eval_ds_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4m79KotvREJ",
        "outputId": "8c89e5cf-c971-4784-b999-cc5fc327a1b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [00:25<00:00, 15.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean per channel: tensor([0.8068, 0.7830, 0.8005])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [00:14<00:00, 26.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standard Deviation per channel: tensor([0.1093, 0.1136, 0.1029])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_df = load_dataframe(ds_dir, 'train')\n",
        "train_transforms = T.Compose([\n",
        "    T.ToTensor()\n",
        "])\n",
        "train_dataset = ImageDataset(train_df, ds_dir / 'train', transform=train_transforms)\n",
        "dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True,\n",
        "                        num_workers=6, pin_memory=True)\n",
        "\n",
        "# Initialize variables to calculate mean\n",
        "mean = torch.zeros(3)  # For RGB channels\n",
        "total_pixels = 0\n",
        "\n",
        "# Loop through the dataset\n",
        "for images, _ in tqdm(dataloader):\n",
        "    # Sum pixel values per channel\n",
        "    mean += images.sum(dim=[0, 2, 3])\n",
        "    total_pixels += images.size(0) * images.size(2) * images.size(3)\n",
        "\n",
        "# Divide by total number of pixels\n",
        "mean /= total_pixels\n",
        "\n",
        "print(f\"Mean per channel: {mean}\")\n",
        "\n",
        "# Initialize variables for std calculation\n",
        "std = torch.zeros(3)\n",
        "\n",
        "# Loop again for standard deviation\n",
        "for images, _ in tqdm(dataloader):\n",
        "    std += ((images - mean.view(1, 3, 1, 1))**2).sum(dim=[0, 2, 3])\n",
        "\n",
        "std = torch.sqrt(std / total_pixels)\n",
        "\n",
        "print(f\"Standard Deviation per channel: {std}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n-yCAoLQprGc"
      },
      "outputs": [],
      "source": [
        "train_df = load_dataframe(ds_dir, 'train')\n",
        "val_df = load_dataframe(ds_dir, 'validation')\n",
        "test_df = load_dataframe(ds_dir, 'test')\n",
        "eval_df = load_dataframe(eval_ds_dir, 'test')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOWecMcbFqlr",
        "outputId": "1852beb5-5414-4b63-a31c-677a4608928e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 3000, 3000, 40000)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_df), len(test_df), len(eval_df), len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=mean, std=std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df['filename'] = test_df['filename'].astype(str).str.strip()\n",
        "for i, fname in enumerate(test_df['filename']):\n",
        "    if '\\n' in fname or ' ' in fname:\n",
        "        print(f\"[WARNING] Bad filename in row {i}: {repr(fname)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HKvxyYIi2PUx"
      },
      "outputs": [],
      "source": [
        "train_dataset = ImageDataset(train_df, ds_dir / 'train', transform=transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                              num_workers=6, pin_memory=True)\n",
        "\n",
        "train_eval_dataset = ImageDataset(train_df, ds_dir / 'train', transform=transform)\n",
        "train_eval_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False,\n",
        "                                   num_workers=6, pin_memory=True)\n",
        "\n",
        "val_dataset = ImageDataset(val_df,  ds_dir / 'validation', transform=transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False,\n",
        "                            num_workers=6, pin_memory=True)\n",
        "\n",
        "test_dataset = ImageDataset(test_df,  ds_dir / 'test', transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                             num_workers=6, pin_memory=True)\n",
        "\n",
        "eval_dataset = ImageDataset(eval_df,  eval_ds_dir / 'test', transform=transform)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False,\n",
        "                             num_workers=6, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = test_dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19huLH3JvPki",
        "outputId": "89e27895-f19a-4f45-919d-da6df6478730"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 3, 128, 128]), torch.Size([32]))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_ex = next(iter(train_dataloader))\n",
        "data_ex[0].shape, data_ex[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJrJOTfzsIof"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I4f0lOt3tap0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV5OA2GDS8nP"
      },
      "source": [
        "## Analysis Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mpOmqqdxsLBa"
      },
      "outputs": [],
      "source": [
        "# create column for absolute sphere difference\n",
        "train_df['sphere_diff'] = np.abs(train_df['spherical'] - train_df['ill_spherical'])\n",
        "val_df['sphere_diff'] = np.abs(val_df['spherical'] - val_df['ill_spherical'])\n",
        "test_df['sphere_diff'] = np.abs(test_df['spherical'] - test_df['ill_spherical'])\n",
        "eval_df['sphere_diff'] = np.abs(eval_df['spherical'] - eval_df['ill_spherical'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiMdu2w_dj2O"
      },
      "source": [
        "# Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_resnet50(num_classes, pretrained=True, checkpoint_path=None):\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)  # Replace final fully-connected layer\n",
        "\n",
        "    if checkpoint_path:\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint)\n",
        "        print(f\"Loaded checkpoint from: {checkpoint_path}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            preds.extend(predicted.cpu().numpy())\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Evaluation Loss: {avg_loss:.4f}, Evaluation Accuracy: {accuracy:.4f}\")\n",
        "    return np.array(preds), avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, dl_train, dl_val, criterion, optimizer, scheduler, device, checkpoint_path, num_epochs=10):\n",
        "    model = model.to(device)\n",
        "    best_val_loss = sys.float_info.max\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for inputs, labels in dl_train:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        train_loss = running_train_loss / len(dl_train)\n",
        "        train_accuracy = correct_train / total_train\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dl_val:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "                total_val += labels.size(0)\n",
        "\n",
        "        val_loss = running_val_loss / len(dl_val)\n",
        "        val_accuracy = correct_val / total_val\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"\\tTrain Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"\\tValidation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch + 1\n",
        "            print(f\"New best model found at epoch {epoch+1} with validation loss: {val_loss:.4f}\")\n",
        "            torch.save(model.state_dict(), checkpoint_path / 'tmp' / 'best_model.pth')\n",
        "\n",
        "    model = load_resnet50(num_classes=len(CLASSES),\n",
        "                          pretrained=False,\n",
        "                          checkpoint_path=checkpoint_path / 'tmp' / 'best_model.pth')\n",
        "    model.to(device)\n",
        "\n",
        "    _, val_loss, val_acc = evaluate_model(model, dl_val, criterion, device)\n",
        "\n",
        "    print(f\"Training Run complete! Val loss = {best_val_loss:.4f} | Val acc = {val_acc:.4f} | Epoch = {best_epoch}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    return model, val_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model path: two4two_sickones_models_pytorch/sick_ones_bendbias_v3_2class_normal/mobilenet\n"
          ]
        }
      ],
      "source": [
        "# setup model path\n",
        "model_path = base_path / ds / f'{modeltype}'\n",
        "model_path.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Model path:\", model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup checkpoint folders\n",
        "checkpoint_path = model_path / \"torch_resnet50/\"\n",
        "(checkpoint_path / 'tmp').mkdir(parents=True, exist_ok=True)\n",
        "(checkpoint_path / 'final').mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training\n",
        "n_runs = 1\n",
        "n_epochs = 50\n",
        "load_checkpoints = False\n",
        "learning_rate = 0.001\n",
        "\n",
        "best_val_loss = sys.float_info.max\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# run \\nfor i in range(n_runs):\\n    set_seed(42 + i)\\n\\n    print(f\"Run {i+1} / {n_runs}\")\\n    print(\"=\" * 30)\\n\\n    if i > 0:\\n        print(\\'loading previous checkpoint with augmentation\\')\\n        load_checkpoints = True\\n\\n    if i >= 0:\\n        print(\\'Loading previous checkpoint and training without augmentation\\')\\n        train_dataset = ImageDataset(train_df, ds_dir / \\'train\\', transform=transform)\\n        train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=6, pin_memory=True)\\n\\n    model = load_resnet50(len(CLASSES), pretrained=False,\\n                          checkpoint_path=checkpoint_path / \\'final\\' / \\'best_model.pth\\' if load_checkpoints else None)\\n\\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=True)\\n\\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\\'min\\', factor=0.4, patience=4,\\n                                                           threshold=0.01, threshold_mode=\\'abs\\')\\n\\n    model, val_loss, val_acc = train_model(model,\\n                                           train_dataloader, val_dataloader,\\n                                           criterion, optimizer, scheduler,\\n                                           device, checkpoint_path,\\n                                           num_epochs=n_epochs)\\n\\n    if val_loss < best_val_loss:\\n        best_val_loss = val_loss\\n        print(f\"New best model found at Run {i+1} with validation loss: {val_loss:.4f}\")\\n        torch.save(model.state_dict(), checkpoint_path / \\'final\\' / \\'best_model.pth\\')\\n    print()\\n\\n#load best model\\nmodel = load_resnet50(num_classes=len(CLASSES),\\n                      pretrained=False,\\n                      checkpoint_path=checkpoint_path / \\'final\\' / \\'best_model.pth\\')\\nmodel.to(device)\\n\\n_, val_loss, val_acc = evaluate_model(model, val_dataloader, criterion, device)\\n\\nprint(f\"Training complete! Val loss = {best_val_loss:.4f} | Val acc = {val_acc:.4f}\")\\nprint(\"-\" * 30)\\n'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run \n",
        "for i in range(n_runs):\n",
        "    set_seed(42 + i)\n",
        "\n",
        "    print(f\"Run {i+1} / {n_runs}\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    if i > 0:\n",
        "        print('loading previous checkpoint with augmentation')\n",
        "        load_checkpoints = True\n",
        "\n",
        "    if i >= 0:\n",
        "        print('Loading previous checkpoint and training without augmentation')\n",
        "        train_dataset = ImageDataset(train_df, ds_dir / 'train', transform=transform)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=6, pin_memory=True)\n",
        "\n",
        "    model = load_resnet50(len(CLASSES), pretrained=False,\n",
        "                          checkpoint_path=checkpoint_path / 'final' / 'best_model.pth' if load_checkpoints else None)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=True)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=4,\n",
        "                                                           threshold=0.01, threshold_mode='abs')\n",
        "\n",
        "    model, val_loss, val_acc = train_model(model,\n",
        "                                           train_dataloader, val_dataloader,\n",
        "                                           criterion, optimizer, scheduler,\n",
        "                                           device, checkpoint_path,\n",
        "                                           num_epochs=n_epochs)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        print(f\"New best model found at Run {i+1} with validation loss: {val_loss:.4f}\")\n",
        "        torch.save(model.state_dict(), checkpoint_path / 'final' / 'best_model.pth')\n",
        "    print()\n",
        "\n",
        "#load best model\n",
        "model = load_resnet50(num_classes=len(CLASSES),\n",
        "                      pretrained=False,\n",
        "                      checkpoint_path=checkpoint_path / 'final' / 'best_model.pth')\n",
        "model.to(device)\n",
        "\n",
        "_, val_loss, val_acc = evaluate_model(model, val_dataloader, criterion, device)\n",
        "\n",
        "print(f\"Training complete! Val loss = {best_val_loss:.4f} | Val acc = {val_acc:.4f}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded checkpoint from: two4two_sickones_models_pytorch/sick_ones_bendbias_v3_2class_normal/mobilenet/torch_resnet50/final/best_model.pth\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\ntrain_preds, _, _ = evaluate_model(model, train_eval_dataloader, criterion, device)\\nevaluate_model(model, val_dataloader, criterion, device)\\ntest_preds, _, _ = evaluate_model(model, test_dataloader, criterion, device)\\neval_preds, _, _ = evaluate_model(model, eval_dataloader, criterion, device)\\n'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load best model and evaluate\n",
        "\n",
        "model = load_resnet50(num_classes=len(CLASSES),\n",
        "                         pretrained=False,\n",
        "                         checkpoint_path=checkpoint_path / 'final' / 'best_model.pth')\n",
        "model.to(device)\n",
        "train_preds, _, _ = evaluate_model(model, train_eval_dataloader, criterion, device)\n",
        "evaluate_model(model, val_dataloader, criterion, device)\n",
        "test_preds, _, _ = evaluate_model(model, test_dataloader, criterion, device)\n",
        "eval_preds, _, _ = evaluate_model(model, eval_dataloader, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNoDN6+oK8F3k9gftoai7WG",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "mount_file_id": "1n5452htjSn6n7eCmFVkp2VlO7Y3M8b8Z",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "hcxai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
