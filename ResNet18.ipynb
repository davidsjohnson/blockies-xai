{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yi0pAex6RNDU"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import tarfile\n",
        "import random\n",
        "\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxTk1ctGpvnW"
      },
      "source": [
        "# Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SLpJb-H9pxs_"
      },
      "outputs": [],
      "source": [
        "CLASSES = ['Healthy', 'OC Degeneration']\n",
        "\n",
        "load_checkpoints = True\n",
        "\n",
        "modeltype = 'mobilenet'\n",
        "\n",
        "ds = 'sick_ones_bendbias_v3_2class_normal'\n",
        "eval_ds = 'sick_ones_bendbias_v3_2class_variation'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua-n5ADsde6b"
      },
      "source": [
        "# Setup and Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSBMiJ-ARVcd",
        "outputId": "01763765-0e0e-4c50-f430-7c4507cc7359"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('ResNet18')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relative_model_path = \"ResNet18\"\n",
        "base_path = Path('./') / relative_model_path\n",
        "base_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HPi0wvbicrct"
      },
      "outputs": [],
      "source": [
        "# data downloading and dataset utilities\n",
        "\n",
        "def download_file(url, file_name, cache_dir=\"data\", extract=True, force_download=False, archive_folder=None):\n",
        "    # Ensure the cache directory exists\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    file_path = os.path.join(cache_dir, file_name)\n",
        "\n",
        "    # Download the file\n",
        "    if not os.path.exists(file_path) or force_download:\n",
        "      torch.hub.download_url_to_file(url, file_path)\n",
        "      print(f\"File downloaded to: {file_path}\")\n",
        "    else:\n",
        "      print(f\"File already exists at: {file_path}\")\n",
        "\n",
        "    if extract:\n",
        "      with tarfile.open(file_path, \"r:gz\") as tar:\n",
        "          tar.extractall(path=cache_dir)\n",
        "      print(f\"File extracted to: {cache_dir}\")\n",
        "      return Path(cache_dir) / archive_folder if archive_folder is not None else Path(cache_dir)\n",
        "    elif archive_folder is not None and (Path(cache_dir) / archive_folder).exists:\n",
        "      return Path(cache_dir) / archive_folder if archive_folder is not None else Path(cache_dir)\n",
        "    else:\n",
        "      return Path(cache_dir)\n",
        "\n",
        "    return Path(file_path)\n",
        "\n",
        "def load_dataframe(data_dir, dataset):\n",
        "  data_dir = data_dir / dataset\n",
        "  df = pd.read_json(data_dir / 'parameters.jsonl', lines=True)\n",
        "  df['filename'] = df['id'] + '.png'\n",
        "  df['ill'] = df['ill'].astype(int).astype(str)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, data_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(str(self.data_dir), str(self.df.iloc[idx]['filename'])) ## Added str \n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = int(self.df.iloc[idx]['ill'])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReNR_Y0QProQ"
      },
      "source": [
        "## Load Dataset and Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a1B47gWYgVG",
        "outputId": "a085949b-646a-403b-db41-3a9b12cdfd77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File already exists at: data/two4two_datasets.tar.gz\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PosixPath('data/two4two_datasets')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download datafrom sciebo\n",
        "\n",
        "data_dir = download_file(\"https://uni-bielefeld.sciebo.de/s/2BgY19ixIaEUOmS/download\",\n",
        "                         \"two4two_datasets.tar.gz\",\n",
        "                         cache_dir='data',\n",
        "                         extract=False,\n",
        "                         force_download=False,\n",
        "                         archive_folder='two4two_datasets')\n",
        "data_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn3D8znPQN-F",
        "outputId": "837838bd-ecf5-4b49-f6ac-94a4e5d5ad5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('data/two4two_datasets/sick_ones_bendbias_v3_2class_normal'),\n",
              " PosixPath('data/two4two_datasets/sick_ones_bendbias_v3_2class_variation'))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_dir = data_dir / ds\n",
        "eval_ds_dir = data_dir / eval_ds\n",
        "ds_dir, eval_ds_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4m79KotvREJ",
        "outputId": "8c89e5cf-c971-4784-b999-cc5fc327a1b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [00:13<00:00, 30.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean per channel: tensor([0.8068, 0.7830, 0.8005])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [00:13<00:00, 30.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standard Deviation per channel: tensor([0.1093, 0.1136, 0.1029])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_df = load_dataframe(ds_dir, 'train')\n",
        "train_transforms = T.Compose([\n",
        "    T.ToTensor()\n",
        "])\n",
        "train_dataset = ImageDataset(train_df, ds_dir / 'train', transform=train_transforms)\n",
        "dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True,\n",
        "                        num_workers=6, pin_memory=True)\n",
        "\n",
        "# Initialize variables to calculate mean\n",
        "mean = torch.zeros(3)  # For RGB channels\n",
        "total_pixels = 0\n",
        "\n",
        "# Loop through the dataset\n",
        "for images, _ in tqdm(dataloader):\n",
        "    # Sum pixel values per channel\n",
        "    mean += images.sum(dim=[0, 2, 3])\n",
        "    total_pixels += images.size(0) * images.size(2) * images.size(3)\n",
        "\n",
        "# Divide by total number of pixels\n",
        "mean /= total_pixels\n",
        "\n",
        "print(f\"Mean per channel: {mean}\")\n",
        "\n",
        "# Initialize variables for std calculation\n",
        "std = torch.zeros(3)\n",
        "\n",
        "# Loop again for standard deviation\n",
        "for images, _ in tqdm(dataloader):\n",
        "    std += ((images - mean.view(1, 3, 1, 1))**2).sum(dim=[0, 2, 3])\n",
        "\n",
        "std = torch.sqrt(std / total_pixels)\n",
        "\n",
        "print(f\"Standard Deviation per channel: {std}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "n-yCAoLQprGc"
      },
      "outputs": [],
      "source": [
        "train_df = load_dataframe(ds_dir, 'train')\n",
        "val_df = load_dataframe(ds_dir, 'validation')\n",
        "test_df = load_dataframe(ds_dir, 'test')\n",
        "eval_df = load_dataframe(eval_ds_dir, 'test')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOWecMcbFqlr",
        "outputId": "1852beb5-5414-4b63-a31c-677a4608928e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 3000, 3000, 40000)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_df), len(test_df), len(eval_df), len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=mean, std=std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df['filename'] = test_df['filename'].astype(str).str.strip()\n",
        "for i, fname in enumerate(test_df['filename']):\n",
        "    if '\\n' in fname or ' ' in fname:\n",
        "        print(f\"[WARNING] Bad filename in row {i}: {repr(fname)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "HKvxyYIi2PUx"
      },
      "outputs": [],
      "source": [
        "train_dataset = ImageDataset(train_df, ds_dir / 'train', transform=transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                              num_workers=6, pin_memory=True)\n",
        "\n",
        "train_eval_dataset = ImageDataset(train_df, ds_dir / 'train', transform=transform)\n",
        "train_eval_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False,\n",
        "                                   num_workers=6, pin_memory=True)\n",
        "\n",
        "val_dataset = ImageDataset(val_df,  ds_dir / 'validation', transform=transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False,\n",
        "                            num_workers=6, pin_memory=True)\n",
        "\n",
        "test_dataset = ImageDataset(test_df,  ds_dir / 'test', transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                             num_workers=6, pin_memory=True)\n",
        "\n",
        "eval_dataset = ImageDataset(eval_df,  eval_ds_dir / 'test', transform=transform)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False,\n",
        "                             num_workers=6, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = test_dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19huLH3JvPki",
        "outputId": "89e27895-f19a-4f45-919d-da6df6478730"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 3, 128, 128]), torch.Size([32]))"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_ex = next(iter(train_dataloader))\n",
        "data_ex[0].shape, data_ex[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJrJOTfzsIof"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "I4f0lOt3tap0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV5OA2GDS8nP"
      },
      "source": [
        "## Analysis Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mpOmqqdxsLBa"
      },
      "outputs": [],
      "source": [
        "# create column for absolute sphere difference\n",
        "train_df['sphere_diff'] = np.abs(train_df['spherical'] - train_df['ill_spherical'])\n",
        "val_df['sphere_diff'] = np.abs(val_df['spherical'] - val_df['ill_spherical'])\n",
        "test_df['sphere_diff'] = np.abs(test_df['spherical'] - test_df['ill_spherical'])\n",
        "eval_df['sphere_diff'] = np.abs(eval_df['spherical'] - eval_df['ill_spherical'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiMdu2w_dj2O"
      },
      "source": [
        "# Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_resnet18(num_classes, pretrained=True, checkpoint_path=None):\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    if checkpoint_path:\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint)\n",
        "        print(f\"Loaded checkpoint from: {checkpoint_path}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            preds.extend(predicted.cpu().numpy())\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Evaluation Loss: {avg_loss:.4f}, Evaluation Accuracy: {accuracy:.4f}\")\n",
        "    return np.array(preds), avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, dl_train, dl_val, criterion, optimizer, scheduler, device, checkpoint_path, num_epochs=10):\n",
        "    model = model.to(device)\n",
        "    best_val_loss = sys.float_info.max\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for inputs, labels in dl_train:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        train_loss = running_train_loss / len(dl_train)\n",
        "        train_accuracy = correct_train / total_train\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dl_val:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "                total_val += labels.size(0)\n",
        "\n",
        "        val_loss = running_val_loss / len(dl_val)\n",
        "        val_accuracy = correct_val / total_val\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"\\tTrain Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"\\tValidation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch + 1\n",
        "            print(f\"New best model found at epoch {epoch+1} with validation loss: {val_loss:.4f}\")\n",
        "            torch.save(model.state_dict(), checkpoint_path / 'tmp' / 'best_model.pth')\n",
        "\n",
        "    model = load_resnet18(num_classes=len(CLASSES),\n",
        "                          pretrained=False,\n",
        "                          checkpoint_path=checkpoint_path / 'tmp' / 'best_model.pth')\n",
        "    model.to(device)\n",
        "\n",
        "    _, val_loss, val_acc = evaluate_model(model, dl_val, criterion, device)\n",
        "\n",
        "    print(f\"Training Run complete! Val loss = {best_val_loss:.4f} | Val acc = {val_acc:.4f} | Epoch = {best_epoch}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    return model, val_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model path: ResNet18/sick_ones_bendbias_v3_2class_normal/mobilenet\n"
          ]
        }
      ],
      "source": [
        "# setup model path\n",
        "model_path = base_path / ds / f'{modeltype}'\n",
        "model_path.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Model path:\", model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup checkpoint folders\n",
        "checkpoint_path = model_path / \"torch_resnet18/\"\n",
        "(checkpoint_path / 'tmp').mkdir(parents=True, exist_ok=True)\n",
        "(checkpoint_path / 'final').mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training\n",
        "n_runs = 2\n",
        "n_epochs = 50\n",
        "load_checkpoints = False\n",
        "learning_rate = 0.001\n",
        "\n",
        "best_val_loss = sys.float_info.max\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run 1 / 2\n",
            "==============================\n",
            "Loading previous checkpoint and training without augmentation\n",
            "Epoch [1/50]\n",
            "\tTrain Loss: 0.5976, Train Accuracy: 0.6818\n",
            "\tValidation Loss: 0.7853, Validation Accuracy: 0.5910\n",
            "New best model found at epoch 1 with validation loss: 0.7853\n",
            "Epoch [2/50]\n",
            "\tTrain Loss: 0.5342, Train Accuracy: 0.7310\n",
            "\tValidation Loss: 0.4978, Validation Accuracy: 0.7600\n",
            "New best model found at epoch 2 with validation loss: 0.4978\n",
            "Epoch [3/50]\n",
            "\tTrain Loss: 0.4975, Train Accuracy: 0.7560\n",
            "\tValidation Loss: 0.4694, Validation Accuracy: 0.7560\n",
            "New best model found at epoch 3 with validation loss: 0.4694\n",
            "Epoch [4/50]\n",
            "\tTrain Loss: 0.4709, Train Accuracy: 0.7722\n",
            "\tValidation Loss: 0.4435, Validation Accuracy: 0.7780\n",
            "New best model found at epoch 4 with validation loss: 0.4435\n",
            "Epoch [5/50]\n",
            "\tTrain Loss: 0.4485, Train Accuracy: 0.7845\n",
            "\tValidation Loss: 0.4603, Validation Accuracy: 0.7690\n",
            "Epoch [6/50]\n",
            "\tTrain Loss: 0.4325, Train Accuracy: 0.7923\n",
            "\tValidation Loss: 0.4126, Validation Accuracy: 0.7910\n",
            "New best model found at epoch 6 with validation loss: 0.4126\n",
            "Epoch [7/50]\n",
            "\tTrain Loss: 0.4185, Train Accuracy: 0.8002\n",
            "\tValidation Loss: 0.4387, Validation Accuracy: 0.7860\n",
            "Epoch [8/50]\n",
            "\tTrain Loss: 0.4103, Train Accuracy: 0.8049\n",
            "\tValidation Loss: 0.4281, Validation Accuracy: 0.7910\n",
            "Epoch [9/50]\n",
            "\tTrain Loss: 0.3997, Train Accuracy: 0.8121\n",
            "\tValidation Loss: 0.4331, Validation Accuracy: 0.7790\n",
            "Epoch [10/50]\n",
            "\tTrain Loss: 0.3897, Train Accuracy: 0.8159\n",
            "\tValidation Loss: 0.3891, Validation Accuracy: 0.8230\n",
            "New best model found at epoch 10 with validation loss: 0.3891\n",
            "Epoch [11/50]\n",
            "\tTrain Loss: 0.3856, Train Accuracy: 0.8190\n",
            "\tValidation Loss: 0.4460, Validation Accuracy: 0.8010\n",
            "Epoch [12/50]\n",
            "\tTrain Loss: 0.3754, Train Accuracy: 0.8246\n",
            "\tValidation Loss: 0.3730, Validation Accuracy: 0.8150\n",
            "New best model found at epoch 12 with validation loss: 0.3730\n",
            "Epoch [13/50]\n",
            "\tTrain Loss: 0.3698, Train Accuracy: 0.8266\n",
            "\tValidation Loss: 0.3734, Validation Accuracy: 0.8100\n",
            "Epoch [14/50]\n",
            "\tTrain Loss: 0.3613, Train Accuracy: 0.8314\n",
            "\tValidation Loss: 0.3734, Validation Accuracy: 0.8220\n",
            "Epoch [15/50]\n",
            "\tTrain Loss: 0.3625, Train Accuracy: 0.8312\n",
            "\tValidation Loss: 0.4133, Validation Accuracy: 0.8190\n",
            "Epoch [16/50]\n",
            "\tTrain Loss: 0.3528, Train Accuracy: 0.8362\n",
            "\tValidation Loss: 0.3738, Validation Accuracy: 0.8170\n",
            "Epoch [17/50]\n",
            "\tTrain Loss: 0.3473, Train Accuracy: 0.8385\n",
            "\tValidation Loss: 0.3613, Validation Accuracy: 0.8170\n",
            "New best model found at epoch 17 with validation loss: 0.3613\n",
            "Epoch [18/50]\n",
            "\tTrain Loss: 0.3437, Train Accuracy: 0.8417\n",
            "\tValidation Loss: 0.3578, Validation Accuracy: 0.8450\n",
            "New best model found at epoch 18 with validation loss: 0.3578\n",
            "Epoch [19/50]\n",
            "\tTrain Loss: 0.3376, Train Accuracy: 0.8474\n",
            "\tValidation Loss: 0.3439, Validation Accuracy: 0.8400\n",
            "New best model found at epoch 19 with validation loss: 0.3439\n",
            "Epoch [20/50]\n",
            "\tTrain Loss: 0.3324, Train Accuracy: 0.8491\n",
            "\tValidation Loss: 0.4158, Validation Accuracy: 0.8150\n",
            "Epoch [21/50]\n",
            "\tTrain Loss: 0.3287, Train Accuracy: 0.8510\n",
            "\tValidation Loss: 0.3301, Validation Accuracy: 0.8510\n",
            "New best model found at epoch 21 with validation loss: 0.3301\n",
            "Epoch [22/50]\n",
            "\tTrain Loss: 0.3246, Train Accuracy: 0.8528\n",
            "\tValidation Loss: 0.3533, Validation Accuracy: 0.8490\n",
            "Epoch [23/50]\n",
            "\tTrain Loss: 0.3174, Train Accuracy: 0.8588\n",
            "\tValidation Loss: 0.3455, Validation Accuracy: 0.8490\n",
            "Epoch [24/50]\n",
            "\tTrain Loss: 0.3154, Train Accuracy: 0.8587\n",
            "\tValidation Loss: 0.3269, Validation Accuracy: 0.8540\n",
            "New best model found at epoch 24 with validation loss: 0.3269\n",
            "Epoch [25/50]\n",
            "\tTrain Loss: 0.3081, Train Accuracy: 0.8644\n",
            "\tValidation Loss: 0.3232, Validation Accuracy: 0.8570\n",
            "New best model found at epoch 25 with validation loss: 0.3232\n",
            "Epoch [26/50]\n",
            "\tTrain Loss: 0.3066, Train Accuracy: 0.8625\n",
            "\tValidation Loss: 0.3365, Validation Accuracy: 0.8530\n",
            "Epoch [27/50]\n",
            "\tTrain Loss: 0.2566, Train Accuracy: 0.8901\n",
            "\tValidation Loss: 0.2937, Validation Accuracy: 0.8690\n",
            "New best model found at epoch 27 with validation loss: 0.2937\n",
            "Epoch [28/50]\n",
            "\tTrain Loss: 0.2461, Train Accuracy: 0.8944\n",
            "\tValidation Loss: 0.2933, Validation Accuracy: 0.8750\n",
            "New best model found at epoch 28 with validation loss: 0.2933\n",
            "Epoch [29/50]\n",
            "\tTrain Loss: 0.2372, Train Accuracy: 0.8976\n",
            "\tValidation Loss: 0.2981, Validation Accuracy: 0.8740\n",
            "Epoch [30/50]\n",
            "\tTrain Loss: 0.2299, Train Accuracy: 0.9034\n",
            "\tValidation Loss: 0.2766, Validation Accuracy: 0.8730\n",
            "New best model found at epoch 30 with validation loss: 0.2766\n",
            "Epoch [31/50]\n",
            "\tTrain Loss: 0.2229, Train Accuracy: 0.9045\n",
            "\tValidation Loss: 0.3004, Validation Accuracy: 0.8820\n",
            "Epoch [32/50]\n",
            "\tTrain Loss: 0.2188, Train Accuracy: 0.9073\n",
            "\tValidation Loss: 0.2985, Validation Accuracy: 0.8730\n",
            "Epoch [33/50]\n",
            "\tTrain Loss: 0.2130, Train Accuracy: 0.9081\n",
            "\tValidation Loss: 0.2859, Validation Accuracy: 0.8910\n",
            "Epoch [34/50]\n",
            "\tTrain Loss: 0.2058, Train Accuracy: 0.9151\n",
            "\tValidation Loss: 0.2929, Validation Accuracy: 0.8640\n",
            "Epoch [35/50]\n",
            "\tTrain Loss: 0.2020, Train Accuracy: 0.9149\n",
            "\tValidation Loss: 0.3346, Validation Accuracy: 0.8680\n",
            "Epoch [36/50]\n",
            "\tTrain Loss: 0.1668, Train Accuracy: 0.9328\n",
            "\tValidation Loss: 0.3097, Validation Accuracy: 0.8850\n",
            "Epoch [37/50]\n",
            "\tTrain Loss: 0.1540, Train Accuracy: 0.9370\n",
            "\tValidation Loss: 0.3131, Validation Accuracy: 0.8730\n",
            "Epoch [38/50]\n",
            "\tTrain Loss: 0.1484, Train Accuracy: 0.9399\n",
            "\tValidation Loss: 0.3086, Validation Accuracy: 0.8790\n",
            "Epoch [39/50]\n",
            "\tTrain Loss: 0.1422, Train Accuracy: 0.9434\n",
            "\tValidation Loss: 0.3344, Validation Accuracy: 0.8780\n",
            "Epoch [40/50]\n",
            "\tTrain Loss: 0.1342, Train Accuracy: 0.9455\n",
            "\tValidation Loss: 0.3333, Validation Accuracy: 0.8860\n",
            "Epoch [41/50]\n",
            "\tTrain Loss: 0.1112, Train Accuracy: 0.9564\n",
            "\tValidation Loss: 0.3452, Validation Accuracy: 0.8760\n",
            "Epoch [42/50]\n",
            "\tTrain Loss: 0.1062, Train Accuracy: 0.9591\n",
            "\tValidation Loss: 0.3502, Validation Accuracy: 0.8710\n",
            "Epoch [43/50]\n",
            "\tTrain Loss: 0.1038, Train Accuracy: 0.9606\n",
            "\tValidation Loss: 0.3600, Validation Accuracy: 0.8700\n",
            "Epoch [44/50]\n",
            "\tTrain Loss: 0.0986, Train Accuracy: 0.9631\n",
            "\tValidation Loss: 0.3743, Validation Accuracy: 0.8750\n",
            "Epoch [45/50]\n",
            "\tTrain Loss: 0.0957, Train Accuracy: 0.9633\n",
            "\tValidation Loss: 0.3629, Validation Accuracy: 0.8810\n",
            "Epoch [46/50]\n",
            "\tTrain Loss: 0.0869, Train Accuracy: 0.9672\n",
            "\tValidation Loss: 0.3701, Validation Accuracy: 0.8760\n",
            "Epoch [47/50]\n",
            "\tTrain Loss: 0.0855, Train Accuracy: 0.9679\n",
            "\tValidation Loss: 0.3875, Validation Accuracy: 0.8710\n",
            "Epoch [48/50]\n",
            "\tTrain Loss: 0.0816, Train Accuracy: 0.9708\n",
            "\tValidation Loss: 0.3941, Validation Accuracy: 0.8720\n",
            "Epoch [49/50]\n",
            "\tTrain Loss: 0.0801, Train Accuracy: 0.9708\n",
            "\tValidation Loss: 0.4005, Validation Accuracy: 0.8710\n",
            "Epoch [50/50]\n",
            "\tTrain Loss: 0.0808, Train Accuracy: 0.9701\n",
            "\tValidation Loss: 0.3963, Validation Accuracy: 0.8740\n",
            "Loaded checkpoint from: ResNet18/sick_ones_bendbias_v3_2class_normal/mobilenet/torch_resnet18/tmp/best_model.pth\n",
            "Evaluation Loss: 0.2766, Evaluation Accuracy: 0.8730\n",
            "Training Run complete! Val loss = 0.2766 | Val acc = 0.8730 | Epoch = 30\n",
            "------------------------------\n",
            "New best model found at Run 1 with validation loss: 0.2766\n",
            "\n",
            "Run 2 / 2\n",
            "==============================\n",
            "loading previous checkpoint with augmentation\n",
            "Loading previous checkpoint and training without augmentation\n",
            "Loaded checkpoint from: ResNet18/sick_ones_bendbias_v3_2class_normal/mobilenet/torch_resnet18/final/best_model.pth\n",
            "Epoch [1/50]\n",
            "\tTrain Loss: 0.2879, Train Accuracy: 0.8731\n",
            "\tValidation Loss: 0.3756, Validation Accuracy: 0.8400\n",
            "New best model found at epoch 1 with validation loss: 0.3756\n",
            "Epoch [2/50]\n",
            "\tTrain Loss: 0.2946, Train Accuracy: 0.8675\n",
            "\tValidation Loss: 0.3450, Validation Accuracy: 0.8390\n",
            "New best model found at epoch 2 with validation loss: 0.3450\n",
            "Epoch [3/50]\n",
            "\tTrain Loss: 0.2905, Train Accuracy: 0.8702\n",
            "\tValidation Loss: 0.3117, Validation Accuracy: 0.8680\n",
            "New best model found at epoch 3 with validation loss: 0.3117\n",
            "Epoch [4/50]\n",
            "\tTrain Loss: 0.2845, Train Accuracy: 0.8759\n",
            "\tValidation Loss: 0.3427, Validation Accuracy: 0.8450\n",
            "Epoch [5/50]\n",
            "\tTrain Loss: 0.2822, Train Accuracy: 0.8780\n",
            "\tValidation Loss: 0.3141, Validation Accuracy: 0.8570\n",
            "Epoch [6/50]\n",
            "\tTrain Loss: 0.2811, Train Accuracy: 0.8775\n",
            "\tValidation Loss: 0.3622, Validation Accuracy: 0.8450\n",
            "Epoch [7/50]\n",
            "\tTrain Loss: 0.2791, Train Accuracy: 0.8769\n",
            "\tValidation Loss: 0.3120, Validation Accuracy: 0.8760\n",
            "Epoch [8/50]\n",
            "\tTrain Loss: 0.2744, Train Accuracy: 0.8807\n",
            "\tValidation Loss: 0.3424, Validation Accuracy: 0.8550\n",
            "Epoch [9/50]\n",
            "\tTrain Loss: 0.2258, Train Accuracy: 0.9038\n",
            "\tValidation Loss: 0.3040, Validation Accuracy: 0.8760\n",
            "New best model found at epoch 9 with validation loss: 0.3040\n",
            "Epoch [10/50]\n",
            "\tTrain Loss: 0.2083, Train Accuracy: 0.9124\n",
            "\tValidation Loss: 0.2800, Validation Accuracy: 0.8790\n",
            "New best model found at epoch 10 with validation loss: 0.2800\n",
            "Epoch [11/50]\n",
            "\tTrain Loss: 0.2049, Train Accuracy: 0.9150\n",
            "\tValidation Loss: 0.2840, Validation Accuracy: 0.8810\n",
            "Epoch [12/50]\n",
            "\tTrain Loss: 0.1942, Train Accuracy: 0.9180\n",
            "\tValidation Loss: 0.3089, Validation Accuracy: 0.8720\n",
            "Epoch [13/50]\n",
            "\tTrain Loss: 0.1943, Train Accuracy: 0.9179\n",
            "\tValidation Loss: 0.2925, Validation Accuracy: 0.8870\n",
            "Epoch [14/50]\n",
            "\tTrain Loss: 0.1872, Train Accuracy: 0.9223\n",
            "\tValidation Loss: 0.2905, Validation Accuracy: 0.8790\n",
            "Epoch [15/50]\n",
            "\tTrain Loss: 0.1853, Train Accuracy: 0.9234\n",
            "\tValidation Loss: 0.3185, Validation Accuracy: 0.8790\n",
            "Epoch [16/50]\n",
            "\tTrain Loss: 0.1458, Train Accuracy: 0.9417\n",
            "\tValidation Loss: 0.2900, Validation Accuracy: 0.8820\n",
            "Epoch [17/50]\n",
            "\tTrain Loss: 0.1380, Train Accuracy: 0.9438\n",
            "\tValidation Loss: 0.3105, Validation Accuracy: 0.8770\n",
            "Epoch [18/50]\n",
            "\tTrain Loss: 0.1318, Train Accuracy: 0.9466\n",
            "\tValidation Loss: 0.3090, Validation Accuracy: 0.8740\n",
            "Epoch [19/50]\n",
            "\tTrain Loss: 0.1271, Train Accuracy: 0.9485\n",
            "\tValidation Loss: 0.3074, Validation Accuracy: 0.8820\n",
            "Epoch [20/50]\n",
            "\tTrain Loss: 0.1223, Train Accuracy: 0.9512\n",
            "\tValidation Loss: 0.3125, Validation Accuracy: 0.8860\n",
            "Epoch [21/50]\n",
            "\tTrain Loss: 0.1043, Train Accuracy: 0.9595\n",
            "\tValidation Loss: 0.3337, Validation Accuracy: 0.8760\n",
            "Epoch [22/50]\n",
            "\tTrain Loss: 0.0978, Train Accuracy: 0.9620\n",
            "\tValidation Loss: 0.3302, Validation Accuracy: 0.8760\n",
            "Epoch [23/50]\n",
            "\tTrain Loss: 0.0969, Train Accuracy: 0.9620\n",
            "\tValidation Loss: 0.3467, Validation Accuracy: 0.8700\n",
            "Epoch [24/50]\n",
            "\tTrain Loss: 0.0944, Train Accuracy: 0.9627\n",
            "\tValidation Loss: 0.3492, Validation Accuracy: 0.8800\n",
            "Epoch [25/50]\n",
            "\tTrain Loss: 0.0913, Train Accuracy: 0.9651\n",
            "\tValidation Loss: 0.3531, Validation Accuracy: 0.8810\n",
            "Epoch [26/50]\n",
            "\tTrain Loss: 0.0801, Train Accuracy: 0.9697\n",
            "\tValidation Loss: 0.3706, Validation Accuracy: 0.8750\n",
            "Epoch [27/50]\n",
            "\tTrain Loss: 0.0794, Train Accuracy: 0.9703\n",
            "\tValidation Loss: 0.3593, Validation Accuracy: 0.8740\n",
            "Epoch [28/50]\n",
            "\tTrain Loss: 0.0768, Train Accuracy: 0.9714\n",
            "\tValidation Loss: 0.3730, Validation Accuracy: 0.8750\n",
            "Epoch [29/50]\n",
            "\tTrain Loss: 0.0757, Train Accuracy: 0.9714\n",
            "\tValidation Loss: 0.3759, Validation Accuracy: 0.8790\n",
            "Epoch [30/50]\n",
            "\tTrain Loss: 0.0738, Train Accuracy: 0.9710\n",
            "\tValidation Loss: 0.3854, Validation Accuracy: 0.8750\n",
            "Epoch [31/50]\n",
            "\tTrain Loss: 0.0713, Train Accuracy: 0.9731\n",
            "\tValidation Loss: 0.3849, Validation Accuracy: 0.8760\n",
            "Epoch [32/50]\n",
            "\tTrain Loss: 0.0710, Train Accuracy: 0.9729\n",
            "\tValidation Loss: 0.3955, Validation Accuracy: 0.8770\n",
            "Epoch [33/50]\n",
            "\tTrain Loss: 0.0688, Train Accuracy: 0.9747\n",
            "\tValidation Loss: 0.3818, Validation Accuracy: 0.8800\n",
            "Epoch [34/50]\n",
            "\tTrain Loss: 0.0714, Train Accuracy: 0.9731\n",
            "\tValidation Loss: 0.4033, Validation Accuracy: 0.8700\n",
            "Epoch [35/50]\n",
            "\tTrain Loss: 0.0688, Train Accuracy: 0.9737\n",
            "\tValidation Loss: 0.3954, Validation Accuracy: 0.8750\n",
            "Epoch [36/50]\n",
            "\tTrain Loss: 0.0659, Train Accuracy: 0.9758\n",
            "\tValidation Loss: 0.3940, Validation Accuracy: 0.8740\n",
            "Epoch [37/50]\n",
            "\tTrain Loss: 0.0680, Train Accuracy: 0.9749\n",
            "\tValidation Loss: 0.3849, Validation Accuracy: 0.8770\n",
            "Epoch [38/50]\n",
            "\tTrain Loss: 0.0651, Train Accuracy: 0.9767\n",
            "\tValidation Loss: 0.3893, Validation Accuracy: 0.8810\n",
            "Epoch [39/50]\n",
            "\tTrain Loss: 0.0659, Train Accuracy: 0.9752\n",
            "\tValidation Loss: 0.3886, Validation Accuracy: 0.8780\n",
            "Epoch [40/50]\n",
            "\tTrain Loss: 0.0676, Train Accuracy: 0.9747\n",
            "\tValidation Loss: 0.3875, Validation Accuracy: 0.8770\n",
            "Epoch [41/50]\n",
            "\tTrain Loss: 0.0660, Train Accuracy: 0.9761\n",
            "\tValidation Loss: 0.3924, Validation Accuracy: 0.8770\n",
            "Epoch [42/50]\n",
            "\tTrain Loss: 0.0649, Train Accuracy: 0.9761\n",
            "\tValidation Loss: 0.4101, Validation Accuracy: 0.8750\n",
            "Epoch [43/50]\n",
            "\tTrain Loss: 0.0664, Train Accuracy: 0.9755\n",
            "\tValidation Loss: 0.3964, Validation Accuracy: 0.8770\n",
            "Epoch [44/50]\n",
            "\tTrain Loss: 0.0666, Train Accuracy: 0.9748\n",
            "\tValidation Loss: 0.4047, Validation Accuracy: 0.8770\n",
            "Epoch [45/50]\n",
            "\tTrain Loss: 0.0672, Train Accuracy: 0.9743\n",
            "\tValidation Loss: 0.3940, Validation Accuracy: 0.8780\n",
            "Epoch [46/50]\n",
            "\tTrain Loss: 0.0659, Train Accuracy: 0.9754\n",
            "\tValidation Loss: 0.3845, Validation Accuracy: 0.8800\n",
            "Epoch [47/50]\n",
            "\tTrain Loss: 0.0665, Train Accuracy: 0.9757\n",
            "\tValidation Loss: 0.3949, Validation Accuracy: 0.8760\n",
            "Epoch [48/50]\n",
            "\tTrain Loss: 0.0645, Train Accuracy: 0.9765\n",
            "\tValidation Loss: 0.3918, Validation Accuracy: 0.8780\n",
            "Epoch [49/50]\n",
            "\tTrain Loss: 0.0643, Train Accuracy: 0.9761\n",
            "\tValidation Loss: 0.3979, Validation Accuracy: 0.8760\n",
            "Epoch [50/50]\n",
            "\tTrain Loss: 0.0675, Train Accuracy: 0.9749\n",
            "\tValidation Loss: 0.3938, Validation Accuracy: 0.8740\n",
            "Loaded checkpoint from: ResNet18/sick_ones_bendbias_v3_2class_normal/mobilenet/torch_resnet18/tmp/best_model.pth\n",
            "Evaluation Loss: 0.2800, Evaluation Accuracy: 0.8790\n",
            "Training Run complete! Val loss = 0.2800 | Val acc = 0.8790 | Epoch = 10\n",
            "------------------------------\n",
            "\n",
            "Loaded checkpoint from: ResNet18/sick_ones_bendbias_v3_2class_normal/mobilenet/torch_resnet18/final/best_model.pth\n",
            "Evaluation Loss: 0.2766, Evaluation Accuracy: 0.8730\n",
            "Training complete! Val loss = 0.2766 | Val acc = 0.8730\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# run \n",
        "for i in range(n_runs):\n",
        "    set_seed(42 + i)\n",
        "\n",
        "    print(f\"Run {i+1} / {n_runs}\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    if i > 0:\n",
        "        print('loading previous checkpoint with augmentation')\n",
        "        load_checkpoints = True\n",
        "\n",
        "    if i >= 0:\n",
        "        print('Loading previous checkpoint and training without augmentation')\n",
        "        train_dataset = ImageDataset(train_df, ds_dir / 'train', transform=transform)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=6, pin_memory=True)\n",
        "\n",
        "    model = load_resnet18(len(CLASSES), pretrained=False,\n",
        "                          checkpoint_path=checkpoint_path / 'final' / 'best_model.pth' if load_checkpoints else None)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=True)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=4,\n",
        "                                                           threshold=0.01, threshold_mode='abs')\n",
        "\n",
        "    model, val_loss, val_acc = train_model(model,\n",
        "                                           train_dataloader, val_dataloader,\n",
        "                                           criterion, optimizer, scheduler,\n",
        "                                           device, checkpoint_path,\n",
        "                                           num_epochs=n_epochs)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        print(f\"New best model found at Run {i+1} with validation loss: {val_loss:.4f}\")\n",
        "        torch.save(model.state_dict(), checkpoint_path / 'final' / 'best_model.pth')\n",
        "    print()\n",
        "\n",
        "#load best model\n",
        "model = load_resnet18(num_classes=len(CLASSES),\n",
        "                      pretrained=False,\n",
        "                      checkpoint_path=checkpoint_path / 'final' / 'best_model.pth')\n",
        "model.to(device)\n",
        "\n",
        "_, val_loss, val_acc = evaluate_model(model, val_dataloader, criterion, device)\n",
        "\n",
        "print(f\"Training complete! Val loss = {best_val_loss:.4f} | Val acc = {val_acc:.4f}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded checkpoint from: ResNet18/sick_ones_bendbias_v3_2class_normal/mobilenet/torch_resnet18/final/best_model.pth\n",
            "Evaluation Loss: 0.1792, Evaluation Accuracy: 0.9259\n",
            "Evaluation Loss: 0.2766, Evaluation Accuracy: 0.8730\n",
            "Evaluation Loss: 0.2894, Evaluation Accuracy: 0.8673\n",
            "Evaluation Loss: 0.7907, Evaluation Accuracy: 0.6640\n"
          ]
        }
      ],
      "source": [
        "# load best model and evaluate\n",
        "\n",
        "model = load_resnet18(num_classes=len(CLASSES),\n",
        "                         pretrained=False,\n",
        "                         checkpoint_path=checkpoint_path / 'final' / 'best_model.pth')\n",
        "model.to(device)\n",
        "train_preds, _, _ = evaluate_model(model, train_eval_dataloader, criterion, device)\n",
        "evaluate_model(model, val_dataloader, criterion, device)\n",
        "test_preds, _, _ = evaluate_model(model, test_dataloader, criterion, device)\n",
        "eval_preds, _, _ = evaluate_model(model, eval_dataloader, criterion, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNoDN6+oK8F3k9gftoai7WG",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "mount_file_id": "1n5452htjSn6n7eCmFVkp2VlO7Y3M8b8Z",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "hcxai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
