{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rule-Based Explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yi0pAex6RNDU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/seham/miniconda3/envs/blockies-xai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import tarfile\n",
        "import random\n",
        "\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxTk1ctGpvnW"
      },
      "source": [
        "# Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SLpJb-H9pxs_"
      },
      "outputs": [],
      "source": [
        "CLASSES = ['Healthy', 'OC Degeneration']\n",
        "\n",
        "load_checkpoints = True\n",
        "\n",
        "modeltype = 'mobilenet'\n",
        "\n",
        "ds = 'sick_ones_bendbias_v3_2class_normal'\n",
        "eval_ds = 'sick_ones_bendbias_v3_2class_variation'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua-n5ADsde6b"
      },
      "source": [
        "# Setup and Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSBMiJ-ARVcd",
        "outputId": "01763765-0e0e-4c50-f430-7c4507cc7359"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('two4two_sickones_models_pytorch')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relative_model_path = \"two4two_sickones_models_pytorch\"\n",
        "base_path = Path('./') / relative_model_path\n",
        "base_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HPi0wvbicrct"
      },
      "outputs": [],
      "source": [
        "def load_dataframe(data_dir, dataset):\n",
        "  data_dir = data_dir / dataset\n",
        "  df = pd.read_json(data_dir / 'parameters.jsonl', lines=True)\n",
        "  df['filename'] = df['id'] + '.png'\n",
        "  df['ill'] = df['ill'].astype(int).astype(str)\n",
        "\n",
        "  return df\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, data_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(str(self.data_dir), str(self.df.iloc[idx]['filename'])) ## Added str \n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = int(self.df.iloc[idx]['ill'])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReNR_Y0QProQ"
      },
      "source": [
        "## Load Dataset and Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def download_file(url, file_name, cache_dir=\"data\", extract=True, force_download=False, archive_folder=None):\n",
        "    # Ensure the cache directory exists\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    file_path = os.path.join(cache_dir, file_name)\n",
        "\n",
        "    # Download the file\n",
        "    if not os.path.exists(file_path) or force_download:\n",
        "      torch.hub.download_url_to_file(url, file_path)\n",
        "      print(f\"File downloaded to: {file_path}\")\n",
        "    else:\n",
        "      print(f\"File already exists at: {file_path}\")\n",
        "\n",
        "    if extract:\n",
        "      with tarfile.open(file_path, \"r:gz\") as tar:\n",
        "          tar.extractall(path=cache_dir)\n",
        "      print(f\"File extracted to: {cache_dir}\")\n",
        "      return Path(cache_dir) / archive_folder if archive_folder is not None else Path(cache_dir)\n",
        "    elif archive_folder is not None and (Path(cache_dir) / archive_folder).exsists:\n",
        "      return Path(cache_dir) / archive_folder\n",
        "    else:\n",
        "      return Path(cache_dir)\n",
        "\n",
        "    return Path(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a1B47gWYgVG",
        "outputId": "a085949b-646a-403b-db41-3a9b12cdfd77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File already exists at: data/two4two_datasets.tar.gz\n",
            "File extracted to: data\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(PosixPath('data/two4two_datasets/sick_ones_bendbias_v3_2class_normal'),\n",
              " PosixPath('data/two4two_datasets/sick_ones_bendbias_v3_2class_variation'))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_dir = download_file(\"https://uni-bielefeld.sciebo.de/s/2BgY19ixIaEUOmS/download\",\n",
        "                         \"two4two_datasets.tar.gz\",\n",
        "                         cache_dir='data',\n",
        "                         extract=True,\n",
        "                         force_download=False,\n",
        "                         archive_folder='two4two_datasets')\n",
        "\n",
        "ds_dir = data_dir / ds\n",
        "eval_ds_dir = data_dir / eval_ds\n",
        "ds_dir, eval_ds_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4m79KotvREJ",
        "outputId": "8c89e5cf-c971-4784-b999-cc5fc327a1b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [00:20<00:00, 19.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean per channel: tensor([0.8068, 0.7830, 0.8005])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [00:14<00:00, 28.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standard Deviation per channel: tensor([0.1093, 0.1136, 0.1029])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_df = load_dataframe(ds_dir, 'train')\n",
        "train_transforms = T.Compose([\n",
        "    T.ToTensor()\n",
        "])\n",
        "train_dataset = ImageDataset(train_df, ds_dir / 'train', transform=train_transforms)\n",
        "dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True,\n",
        "                        num_workers=6, pin_memory=True)\n",
        "\n",
        "# Initialize variables to calculate mean\n",
        "mean = torch.zeros(3)  # For RGB channels\n",
        "total_pixels = 0\n",
        "\n",
        "# Loop through the dataset\n",
        "for images, _ in tqdm(dataloader):\n",
        "    # Sum pixel values per channel\n",
        "    mean += images.sum(dim=[0, 2, 3])\n",
        "    total_pixels += images.size(0) * images.size(2) * images.size(3)\n",
        "\n",
        "# Divide by total number of pixels\n",
        "mean /= total_pixels\n",
        "\n",
        "print(f\"Mean per channel: {mean}\")\n",
        "\n",
        "# Initialize variables for std calculation\n",
        "std = torch.zeros(3)\n",
        "\n",
        "# Loop again for standard deviation\n",
        "for images, _ in tqdm(dataloader):\n",
        "    std += ((images - mean.view(1, 3, 1, 1))**2).sum(dim=[0, 2, 3])\n",
        "\n",
        "std = torch.sqrt(std / total_pixels)\n",
        "\n",
        "print(f\"Standard Deviation per channel: {std}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n-yCAoLQprGc"
      },
      "outputs": [],
      "source": [
        "train_df = load_dataframe(ds_dir, 'train')\n",
        "val_df = load_dataframe(ds_dir, 'validation')\n",
        "test_df = load_dataframe(ds_dir, 'test')\n",
        "eval_df = load_dataframe(eval_ds_dir, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOWecMcbFqlr",
        "outputId": "1852beb5-5414-4b63-a31c-677a4608928e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 3000, 3000, 40000)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_df), len(test_df), len(eval_df), len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=mean, std=std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df['filename'] = test_df['filename'].astype(str).str.strip()\n",
        "for i, fname in enumerate(test_df['filename']):\n",
        "    if '\\n' in fname or ' ' in fname:\n",
        "        print(f\"[WARNING] Bad filename in row {i}: {repr(fname)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HKvxyYIi2PUx"
      },
      "outputs": [],
      "source": [
        "train_dataset = ImageDataset(train_df, ds_dir / 'train', transform=transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                              num_workers=6, pin_memory=True)\n",
        "\n",
        "train_eval_dataset = ImageDataset(train_df, ds_dir / 'train', transform=transform)\n",
        "train_eval_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False,\n",
        "                                   num_workers=6, pin_memory=True)\n",
        "\n",
        "val_dataset = ImageDataset(val_df,  ds_dir / 'validation', transform=transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False,\n",
        "                            num_workers=6, pin_memory=True)\n",
        "\n",
        "test_dataset = ImageDataset(test_df,  ds_dir / 'test', transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                             num_workers=6, pin_memory=True)\n",
        "\n",
        "eval_dataset = ImageDataset(eval_df,  eval_ds_dir / 'test', transform=transform)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False,\n",
        "                             num_workers=6, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19huLH3JvPki",
        "outputId": "89e27895-f19a-4f45-919d-da6df6478730"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 3, 128, 128]), torch.Size([32]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_ex = next(iter(train_dataloader))\n",
        "data_ex[0].shape, data_ex[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV5OA2GDS8nP"
      },
      "source": [
        "## Analysis Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mpOmqqdxsLBa"
      },
      "outputs": [],
      "source": [
        "# create column for absolute sphere difference\n",
        "train_df['sphere_diff'] = np.abs(train_df['spherical'] - train_df['ill_spherical'])\n",
        "val_df['sphere_diff'] = np.abs(val_df['spherical'] - val_df['ill_spherical'])\n",
        "test_df['sphere_diff'] = np.abs(test_df['spherical'] - test_df['ill_spherical'])\n",
        "eval_df['sphere_diff'] = np.abs(eval_df['spherical'] - eval_df['ill_spherical'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiMdu2w_dj2O"
      },
      "source": [
        "# Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_resnet50(num_classes, pretrained=True, checkpoint_path=None):\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)  # Replace final fully-connected layer\n",
        "\n",
        "    if checkpoint_path:\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint)\n",
        "        print(f\"Loaded checkpoint from: {checkpoint_path}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model path: two4two_sickones_models_pytorch/sick_ones_bendbias_v3_2class_normal/mobilenet\n"
          ]
        }
      ],
      "source": [
        "# setup model path\n",
        "model_path = base_path / ds / f'{modeltype}'\n",
        "model_path.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Model path:\", model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup checkpoint folders\n",
        "checkpoint_path = model_path / \"torch_resnet50/\"\n",
        "(checkpoint_path / 'tmp').mkdir(parents=True, exist_ok=True)\n",
        "(checkpoint_path / 'final').mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded checkpoint from: two4two_sickones_models_pytorch/sick_ones_bendbias_v3_2class_normal/mobilenet/torch_resnet50/final/best_model.pth\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load best model and evaluate\n",
        "\n",
        "model = load_resnet50(num_classes=len(CLASSES),\n",
        "                         pretrained=False,\n",
        "                         checkpoint_path=checkpoint_path / 'final' / 'best_model.pth')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/94 [00:00<?, ?it/s]/home/seham/miniconda3/envs/blockies-xai/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "100%|██████████| 94/94 [00:12<00:00,  7.42it/s]\n"
          ]
        }
      ],
      "source": [
        "# Initialize a list to store predictions\n",
        "model_predictions = []\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "with torch.no_grad():\n",
        "    for images, _ in tqdm(test_dataloader):\n",
        "        # Move images to the same device as the model\n",
        "        images = images.to(device)\n",
        "        \n",
        "        # Get model outputs\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Get predicted class (highest probability)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        \n",
        "        # Append predictions to the list\n",
        "        model_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert predictions to a NumPy array\n",
        "model_predictions = np.array(model_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Anchor Explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from alibi.datasets import load_cats\n",
        "from alibi.explainers import AnchorImage\n",
        "from skimage.segmentation import slic\n",
        "from torchvision.models import inception_v3\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.utils import Bunch\n",
        "from typing import Tuple, Union\n",
        "import torch\n",
        "\n",
        "def load_blockies(dataloader: torch.utils.data.DataLoader,\n",
        "                               image_shape: tuple = (299, 299, 3),\n",
        "                               return_X_y: bool = False\n",
        "                              ) -> Union[Bunch, Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Load the full dataset from a DataLoader, similar to alibi.datasets.load_cats.\n",
        "    \"\"\"\n",
        "    X_list, y_list = [], []\n",
        "    target_size = image_shape[:2]  # (height, width)\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        # Resize images\n",
        "        images = F.interpolate(images, size=target_size, mode='bilinear', align_corners=False)\n",
        "        X_list.append(images.cpu())\n",
        "        y_list.append(labels.cpu())\n",
        "\n",
        "    X = torch.cat(X_list, dim=0).permute(0, 2, 3, 1).numpy()  # NCHW -> NHWC\n",
        "    y = torch.cat(y_list, dim=0).numpy()\n",
        "\n",
        "    if return_X_y:\n",
        "        return X, y\n",
        "    else:\n",
        "        return Bunch(data=X, target=y)\n",
        "\n",
        "\n",
        "\n",
        "data, labels = load_blockies(eval_dataloader, image_shape=(299, 299, 3), return_X_y=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Resize((299, 299)),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "images = torch.stack([transform(img) for img in data]).to(device)\n",
        "\n",
        "# Prediction\n",
        "with torch.no_grad():\n",
        "    preds = model(images)\n",
        "top3 = torch.topk(F.softmax(preds, dim=1), 3)\n",
        "print(top3.indices[0].cpu().numpy())\n",
        "\n",
        "# Wrapper\n",
        "def predict_fn(x):\n",
        "    x = torch.tensor(x).permute(0, 3, 1, 2).float()  # NHWC -> NCHW\n",
        "    x = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(x/255.0)\n",
        "    x = x.to(device)\n",
        "    with torch.no_grad():\n",
        "        preds = model(x)\n",
        "    return preds.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explainer\n",
        "image_shape = (299, 299, 3)\n",
        "segmentation_fn = 'slic'\n",
        "kwargs = {'n_segments': 15, 'compactness': 20, 'sigma': 0.5, 'start_label': 0}\n",
        "explainer = AnchorImage(predict_fn, image_shape, segmentation_fn=segmentation_fn,\n",
        "                        segmentation_kwargs=kwargs, images_background=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explanation\n",
        "i = 0\n",
        "plt.imshow(data[i])\n",
        "\n",
        "image = data[i]  # raw image\n",
        "np.random.seed(0)\n",
        "explanation = explainer.explain(image, threshold=0.95, p_sample=0.5, tau=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization\n",
        "plt.figure()\n",
        "plt.imshow(explanation.anchor)\n",
        "plt.figure()\n",
        "plt.imshow(explanation.segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNoDN6+oK8F3k9gftoai7WG",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "mount_file_id": "1n5452htjSn6n7eCmFVkp2VlO7Y3M8b8Z",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "blockies-xai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
